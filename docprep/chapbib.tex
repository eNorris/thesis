%%
%% This is file `chapbib.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% ths.dtx  (with options: `chapmin,addbib')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from chapbib.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file ths.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)

This chapter summarizes the literature reviewed in preparation of this work. The literature review is split into multiple sections, each highlighting relevant works pertaining to a particular component or methodology behind DOCTORS. The first two sections cover current dose estimation techniques and steps necessary for preprocessing data. Section~\ref{sec:discordlit} covers historical and recent work in discrete ordinate solution methodology, the methodology used by DOCTORS. The remaining sections give an overview of GPU hardware and other implementation facets.

\section{CT Dose Estimation}
As discussed in Chapter 1, the dose a patient receives from both diagnostic CT procedures and radiation therapy is of great concern to the medical community [CITE]. However, dose quantification is not currently patient specific. The air kerma (measured in Gy) measures the x-ray beam intensity and can be measured directly [CITE]. As the x-ray beam passes through the patient, it deposits energy. The energy deposition per unit mass is the absorbed dose (measured in Gy) which is then weighted by the radiation type to give the equivalent dose (measured in Sv). The equivalent dose quantifies the biological detriment to the patient from the procedure. The effective dose (measured in Sv) accounts for the radiosensitivity of each organ and is the most meaningful measure of risk to the patient. Any two procedures that result in the same effective dose to the patient carry the same acute and long-term risks. Any procedure that gives an effective dose to a patient has the same risk as any procedure that would give the same whole body dose.

Unfortunately, neither the absorbed dose, equivalent dose, nor effective dose to a patient can be measured directly. Instead, the CT Dose Index (CTDI) is measured using a pencil shaped ionization chamber at the center of  a standardized acrylic phantom placed at the scanner isocenter~\citep{ref:hudaw}. The CTDI quantifies the equivalent dose the patient receives from a single slice of the CT scan protocol. The CTDI does \textit{not} capture patient-specific papameters, but rather quantifies the amount or radiation emitted by the source.

The CTDI does not provide the dose from an entire procedure, but rather a single slice. The dose from a procedure is obtained from the dose-length-product (DLP) which is the product of the CTDI and the axial scan length. The DLP is related to the effective dose to the patient for a particular procedure by a conversion factor that is dependent on many variables including the anatomical region scanned, the x-ray tube voltage, and the patient's age, gender, and size~\citep{ref:hudaw}.

\citet{ref:lauk} found that severly obese patients received twice as much dose from CT procedures as others on average. Some patients received as much as five times the typical dose.

The need for a complete understanding of the transport and scatter through the patient motivates full transport solutions. Typical solution modalities involve three key steps: (1) interpret the user input and prepare the solver, (2) run the transport solution to compute the scalar flux, and (3) convert the scalar flux to a dose. The first step is completely code dependent. The second and third steps of all major solvers falls into one of a few categories.

Monte Carlo codes have been used in the past to compute flux-to-dose conversion factors for a reference person~\citep{ref:icrp116}. The values provided by~\citet{ref:icrp116} are energy dependent conversion factors, $H(E)$ that convert flux to effective dose for a particular orientation. The dose, $D$ is then computed
\begin{equation}
D = \int_0^\infty \varphi(E) H(E) dE
\end{equation}
where $\varphi$ is the scalar flux.

Monte Carlo codes are generally known for their accuracy; they are able to sample the problem and attain arbitrarily high precision given sufficient runtime. They also take advantage of combinatorial geometry allowing a user to produce geometrically complex 3D structures. Deterministic methods are generally much faster than their Monte Carlo counterparts but do not benefit from excess runtime. They also consume vastly more resources, particularly random-access memory (RAM). However, in regularly structures meshes, deterministic methods can greatly outperform Monte Carlo. A large number of spatial regions slows down the particle transport in Monte Carlo and increases the RAM required to store the numerous voxels.

Monte Carlo codes such as TOPAS and Geant4 \citep{ref:agostinellis} are the gold standard \citep{ref:jiax}

Codes such as EGS4, its improved versions EGSnrc, and EGS5 \citep{ref:nelsonw}, and PENELOPE \citep{ref:salvatf} are all Monte Carlo and have shown very good results to experimental. But they are developed for high energy physics.

The alternative to Monte Carlo is deterministic solutions. The best known solution techniques include the discrete ordinate method, the finite element method, and the method of characteristics. The discrete ordinate method is employed by DOCTORS and the methodology is covered independently in Section~\ref{sec:discordlit}.

One of the most well known MOC codes id DeCART. DeCART is actually a coupled solver in that it utilizes different solution modalities for the radial plane and the axial direction. It uses a more complex modality in the radial plane such as MOC and then uses diffusion to transport particles axially~\citep{ref:hursinm}.

The Method of Characteristics is a deterministic solution that has some advantages over discrete ordinates but is very complex to set up in 3D. One implementation is the TRANSMED code which was used to compute external beam therapy dose profiles accurately~\citep{ref:williamsm}.

The MOC solution has been extended to the time dependent domain by \citet{ref:hoffmana}.

Raytracing algorithms have been developed for numerous discret ordinate codes include RAY3D~\citep{ref:yingz} and ATTILLA~\citep{ref:wareingt} which utilizes unstructured tetrahedral meshes.



Dose estimations have been made using many methodologies including 

COMPUTATION

\section{CT Phantom Generation}

In order for the proposed methodology to be viable, the patient's CT phantom must be available. The phantom is reconstructed from the CT data. Though it is not the focus of this work, a brief summary of reconstruction algorithms is given here.

Fundamentally, all reconstructions algorithms are based on a thin parallel beam of x-rays rotating axially about a patient to produce a sinogram. The inverse radon transform backprojects each row of the sinogram back to physical space. Filtering each row in Fourier space and then compositing all layers together recreates the image [CTIE]. This methodology has been extended to fan beams [CITE] and cone beams [CITE]. After the filtered backprojection, iterative reconstruction techniques can be applied to greatly improve the reconstruction quality [CITE].

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.38\textwidth}
        \includegraphics[width=\textwidth]{figs/backproject1}
        \caption{}
        \label{fig:beamconexy}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.19\textwidth}
        \includegraphics[width=\textwidth]{figs/backproject2}
        \caption{}
        \label{fig:beamfanxy}
    \end{subfigure}
    ~
    \begin{subfigure}[b]{0.38\textwidth}
        \includegraphics[width=\textwidth]{figs/backproject3}
        \caption{}
        \label{fig:subsweep_general3}
    \end{subfigure}
    \caption{(a) The original phantom. (b) The sinogram produced by 360 projections. (c) The filter backprojection reconstruction.}\label{fig:backprojection}
\end{figure}

Along any particular ray, monoenergetic particles will attenuate through a homogenous media according to the Beer-Lambert law given in Eq.~\ref{eq:beer_lambert} [CITE] resulting in a small fraction of the source particles reaching the detector. This phenomena leads to an image of the attenuating media but also results in the detector receiving information about the attenuation coefficient, $\mu$, along that ray. With sufficient rays of varying directionality, the entire object can be quantified with respect to its spatially distributed attenuation coefficient. The Hounsfield units (also called CT numbers) are traditionally scaled such that -1000 is dry air and water is zero [CITE].

\begin{equation}\label{eq:beer_lambert}
I(x) = I_0 e^{-\mu x}
\end{equation}

The reconstruction populates the CT mesh with CT numers, also known as Hounsfield units. These values are related to the attenuation coefficient of the material represented by the voxel. Traditionally, a CT number of zero corresponds to the attenuation of water while -1000 corresponds to dry air CITE though some variations do exist \citep{ref:plessisf} \citep{ref:sawc}.

\citet{ref:schneideru} proposed a method to utilize different phantom materials to calibrate a HU-to-material conversion. He used only six different materials that descriminated between fat, water, muscle, and three densities of bone. From the calibration curve, he computed proton stopping powers accurate to within 1-2\%.

\citet{ref:plessisf} used a methodology similar to Schneider's but instead of performing experiments with phantoms, he used the ITS3 \citep{ref:halbleibj} Monte Carlo code to generate data. He identified 16 major tissue types in the body and classified them into seven dosimetrically equivalent categories. The resulting algorithm transormed HU directly into material and density values. Shortly thereafter, \citet{ref:schneiderw} completed a similar study but proposed considering all materials as a mixture of tissue and bone to varying degrees. However, the proposed algorithm fails to work well in soft tissue regions due to the presence of three components (water, fat, and muscle).

Many of the earlier studies in HU-to-material conversions were done with high energy (typically 6-8 MeV) photon beams used for treatment \citep{ref:kimh} \citep{ref:vanderstraetenb}. \citet{ref:sawc} used a phantom with 17 inserts representing different dosimetric tissues.

\citet{ref:ottossonr} extended the methodology provided by \citet{ref:schneideru} to provide 19 dosimetric groups of materials in the diagnostic energy domain independent of the particular scanner used to generate the data. The DOCTORS code relies on his proposed 19-group model. Table~\ref{table:ctmap} shows the mapping of CT number to material composition (reproduced from \citep{ref:ottossonr}).

\begin{sidewaystable}[ht]
\caption{Water Regions}
\centering 
\begin{tabular}{l c r r r r r r r r r r r r}
\hline \hline   
Media       & CT Range      & H    & C    & N    & O    & Na  & Mg  & P    & S   & Cl  & Ar  & K   & Ca \\ [0.5ex] 
\hline
Air         & -950 to -100  &      &      & 75.7 & 23.3 &     &     &      &     &     & 1.3 &     &      \\
Lung        & -1000 to -950 & 10.3 & 10.5 &  3.1 & 74.9 & 0.2 &     &  0.2 & 0.3 & 0.3 &     & 0.2 &      \\
Adipose     & -100 to 15    & 11.2 & 50.8 &  1.2 & 36.4 & 0.1 &     &      & 0.1 & 0.1 &     &     &      \\
Connective  & 15 to 129     & 10.0 & 16.3 &  4.3 & 68.4 & 0.4 &     &      & 0.4 & 0.3 &     &     &      \\
Bone 1      & 129 to 200    &  9.7 & 44.7 &  2.5 & 35.9 &     &     &  2.3 & 0.2 & 0.1 &     & 1.0 &  4.5 \\ 
Bone 2      & 200 to 300    &  9.1 & 41.4 &  2.7 & 36.8 &     & 0.1 &  3.2 & 0.2 & 0.1 &     & 1.0 &  6.3 \\ 
Bone 3      & 300 to 400    &  8.5 & 37.8 &  2.9 & 37.9 &     & 0.1 &  4.1 & 0.2 & 0.1 &     & 1.0 &  8.2 \\ 
Bone 4      & 400 to 500    &  8.0 & 34.5 &  3.1 & 38.8 &     & 0.1 &  5.0 & 0.2 & 0.1 &     & 1.0 & 10.0 \\ 
Bone 5      & 500 to 600    &  7.5 & 31.6 &  3.2 & 39.7 &     & 0.1 &  5.8 & 0.2 & 0.1 &     &     & 11.6 \\ 
Bone 6      & 600 to 700    &  7.1 & 28.7 &  3.4 & 40.4 &     & 0.1 &  6.6 & 0.2 & 0.1 &     &     & 13.1 \\ 
Bone 7      & 700 to 800    &  6.7 & 26.7 &  3.5 & 41.2 &     & 0.2 &  7.2 & 0.3 &     &     &     & 14.4 \\ 
Bone 8      & 800 to 900    &  6.3 & 24.7 &  3.7 & 41.8 &     & 0.2 &  7.8 & 0.3 &     &     &     & 15.7 \\ 
Bone 9      & 900 to 1000   &  6.0 & 22.7 &  3.8 & 42.4 &     & 0.2 &  8.4 & 0.3 &     &     &     & 16.8 \\ 
Bone 10     & 1000 to 1100  &  5.6 & 20.7 &  3.9 & 43.0 &     & 0.2 &  8.9 & 0.3 &     &     &     & 17.9 \\ 
Bone 11     & 1100 to 1200  &  5.3 & 18.7 &  4.0 & 43.5 &     & 0.2 &  9.4 & 0.3 &     &     &     & 18.9 \\ 
Bone 12     & 1200 to 1300  &  5.1 & 16.7 &  4.1 & 44.0 &     & 0.2 &  9.9 & 0.3 &     &     &     & 19.8 \\
Bone 13     & 1300 to 1400  &  4.8 & 15.7 &  4.2 & 44.4 &     & 0.2 & 10.3 & 0.3 &     &     &     & 20.7 \\
Bone 14     & 1400 to 1500  &  4.6 & 13.7 &  4.2 & 44.9 &     & 0.2 & 10.7 & 0.3 &     &     &     & 21.5 \\  
Bone 15     & > 1500        &  4.3 & 12.7 &  4.3 & 45.3 &     & 0.2 & 11.1 & 0.3 &     &     &     & 22.2 \\[1ex]
\hline
\end{tabular}
\label{table:ctmap}
\end{sidewaystable}

\section{Discrete Ordinates}\label{sec:discordlit}
The dsicrete ordinates method dates back to CITE but remains one of the most prominent solution modalities for ration transport in use today. A comprehensive review of discrete ordinate methods is given by \citet{ref:lewise}. Additional references include [CITES].

The oldest major discrete ordinate method implementation is DORT which was superceded by its 3D counterpart TORT~\citep{ref:rhoadesw}. Since TORT, new discrete ordinate method implementations have laregly been extensions allowing more advanced computation.

Discrete ordinate methods are almost always done on a Cartesian grid, though alternative derivations on general geometry do exist~\citep{ref:dehart}. Extension to unstructured tetrahedral meshes was done by ~\citet{ref:wareingt}.

\citet{ref:waltersw} extended the discrete ordinates method to employ an adaptive collision source. Similarly, \citet{ref:ahrensc} wrote an algorithm that allows the quadrature to adapt to the energy group. Both of these methods improve the angular refinement. \citet{ref:ibrahima} added an adaptive mesh refinement. The adaptive mesh refinement allowed the code to automatically refine areas of interest where the flux was rapidly changing. This reduced the error in those regions reducing the computation time needed to reach a particular uncertainty level. An overview of other multigrid reduction schemes is given by \citet{ref:leeb}. \citet{ref:efremenkod} enumerates additional acceleration techniques available for discrete ordinates that can increase speed by 15-30\%.

Denovo is a massively parallel general-purpose discrete ordinate solver developed by~\citet{ref:evanst} to replace TORT. Denovo uses modern programming standards and utilizes the Exnihilo package for data processing and solution methods~\citep{ref:evanst2}. 

KBA DISCUSSION

\section{Raytracing}

Many raytracing algorithms are based on the Bresenham line raserizing algorithm which was designed for graph plotting hardware~citep{ref:bresenhamj}. That algorithm was intende to produce lines acceptable for human interpretation and no guarantee is made that all pixels along the line will be identified making this class of algorithms inappropriate for dosimetric raytracing since attenuation through some voxels would be skipped. Some extensions have been added to it that guarantee passage through all voxels though~\citep{ref:liuy}.

\citet{ref:clearyj} proposed a voxel traversal algorithm in which they initially identify the voxel containing the starting point of the ray, and transport it to the first surface of its bounding voxel it would cross. This is done by computing the distance traveled in order to cross each enclosing surface. The surface with the least distance is the first surface crossed. Therefore, the particle is advanced to the intersection point and considered inside the next cell, having crossed the surface. This process is repeated until a termination criteria is met.

This work uses the raytracing algorithm proposed by~\citet{ref:wooa} which is very similar to the method proposed by~\citet{ref:clearyj} but based on the parametric equation of a line, $\boldsymbol{u} + \boldsymbol{v}t$ which makes implementation in 3D simple. Since its develpment, many improvements and extensions have been made for alternate geometries such as rhombic dodecahedra~\citep{ref:hel}

GPU RAYTRACING

\section{GPU Acceleration}

Two major GPU manufacturers control the vast majority of the market, Nvidia and AMD [CITE]. In 2007, Nvidia released its Compute Unified Device Architecture (CUDA) language. CUDA allows code written in C/C++/FORTRAN to communicate with a GPU with minimal additional code. Therefore, CUDA was selected for the GPU acceleration of DOCTORS. Therefore, only Nvidia GPUs are considered in this work.

In GPU texts, the GPU itself is referred to as a device and the CPU or other hardward running the GPU is referred to as the host. The device has its own instruction set which requires a special compiler. The device also has its own onboard memory so data must be copied from the host to the device and then back.

Conceptually, code run on the device is split into block on a grid, each block then runs multiple concurrent computations, each on a thread. In hardware, threads execute on 32-128 streaming multiprocessors depending on the GPU architecture.

Monte Carlo dose computation for protons has been accelerated by GPU technology \citep{ref:jiax} and Monte Carlo in voxelized geometries has been done by \citep{ref:hissoinys} for monoenergetic beams.

\endinput
%%
%% End of file `chapbib.tex'.
